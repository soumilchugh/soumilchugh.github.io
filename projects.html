<!DOCTYPE html>
<html>
<head>
  <title>Soumil Chugh - Projects</title>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <style>
    body {
      font-family: Arial, sans-serif;
      background-color: #f2f2f2;
      margin: 0;
      padding: 0;
    }
    #project1img {
  background-image: url('https://soumilchugh.github.io/vr.png');
  background-position: center;
  background-repeat: no-repeat;
  background-size: cover;
  border: 5px solid #f0f0f0;
  margin: 0;
  width: 350px;
  height: 350px;
  vertical-align: middle;
  border-radius: 60%;
  display: block;
  margin-bottom: 15px;
  box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
  transition: all 0.3s ease;
  flex: 1;
  text-align: center;
  padding-right: 20px;
}
    h1 {
      font-size: 36px;
      font-weight: bold;
      text-align: center;
      margin-top: 50px;
    }
    .container {
      max-width: 1200px;
      margin: 0 auto;
      padding: 50px;
    }
    .row {
      display: flex;
      flex-wrap: wrap;
      margin: -10px;
    }
    .col {
      flex: 1;
      padding: 10px;
      margin-bottom: 30px;
    }
    .col img {
      max-width: 300px;
      max-height: 300px;
      display: block;
      margin: 0 auto;
      border-radius: 5px;
      box-shadow: 0 0 10px rgba(0,0,0,0.2);
      transition: transform 0.2s ease-in-out;
    }
    .col img:hover {
      transform: scale(1.05);
      cursor: pointer;
      box-shadow: 0 0 20px rgba(0,0,0,0.3);
    }
    .col h3 {
      font-size: 24px;
      font-weight: bold;
      margin-top: 20px;
      margin-bottom: 10px;
    }
    .col p {
      font-size: 16px;
      line-height: 1.5;
      margin-bottom: 20px;
    }
    @media screen and (max-width: 768px) {
      .col {
        flex-basis: 50%;
      }
    }
    @media screen and (max-width: 576px) {
      .col {
        flex-basis: 100%;
      }
    }
  </style>
</head>
<body>
  <h1>Projects</h1>
  <div class="container">
    <div class="row">
      
      <div class="row">
      <div class="col">
        
        <h3>Eye Tracker for a Car</h3>
        <a href="project1.html">
          <img src="https://soumilchugh.github.io/car.jpeg" alt="Project 3",id="project1img">
        </a>
        <p>
          Driver monitoring systems are increasingly essential components of autonomous and semi-autonomous vehicles. Central to these systems is eye-tracking technology, which plays a crucial role in assessing and maintaining driver attentiveness. Numerous patented algorithms have been developed to facilitate the seamless integration of such monitoring systems within vehicles, ensuring enhanced safety and performance.
          In addition to eye tracking, these advanced driver monitoring systems can incorporate other biometric and behavioural data, such as facial expressions, head movements, and even physiological signals, to provide a comprehensive assessment of the driver's state. This multi-modal approach contributes to a more reliable and effective system, enhancing overall road safety and facilitating the transition to autonomous driving technologies.
        </p>
      </div>
      <div class="col">
        
        <h3>Smartwatch Software designed to collect biomarker data</h3>
        <a href="project2.html">
          <img src="https://soumilchugh.github.io/watch.jpeg" alt="Project 4",id="project1img">
        </a>
        <p>
          Utilizing biomarker data, including step count, heart rate, and heart rate variability (HRV), reveals numerous concealed patterns and exhibits strong correlations with an array of blood biomarkers. The objective of this project was to develop a custom software solution capable of unobtrusively recording this data and transmitting it to the cloud. Furthermore, the platform incorporates supplementary watch features to enhance its functionality. 
          Additionally, the custom software solution provides an intuitive user interface, allowing for easy access to the recorded data and facilitating more informed health-related decisions. The integration of advanced analytics also enables the identification of trends and potential health risks, ultimately improving user well-being.
        </p>
      </div>
    </div>


    <div class="row">
      <div class="col">
        
        <h3>Machine Learning Framework for Analyzing Smartwatch Data and Blood Biomarkers</h3>
        <a href="project1.html">
          <img src="https://soumilchugh.github.io/model.png" alt="Project 3",id="project1img">
        </a>
        <p>
          The objective of this project is to develop a cutting-edge machine-learning framework designed to uncover patterns and correlations between data collected from smartwatches and various blood biomarkers. By analyzing biomarker data, such as step count, heart rate, and heart rate variability (HRV), the framework will facilitate a deeper understanding of the relationships between wearable device data and specific health indicators.
          Leveraging advanced machine learning algorithms, the framework will process large volumes of smartwatch data and blood biomarker information, identifying meaningful connections and potential trends. The insights gained from this analysis can contribute to more informed health-related decisions, improved personalized healthcare, and early detection of potential health risks.
          Ultimately, this project aims to create a powerful tool that can not only enhance the utility of smartwatches for health monitoring but also advance the field of personalized medicine and preventive healthcare.
        </p>
      </div>
      <div class="col">
        
        <h3>Computer Vision System for Analyzing At-Home Blood Sampling Cards</h3>
        <a href="project2.html">
          <img src="https://soumilchugh.github.io/blood-card.jpeg" alt="Project 4",id="project1img">
        </a>
        <p>
          The goal of this project is to develop an innovative computer vision system that can analyze at-home blood sampling cards to determine the accuracy and quality of blood collection performed by patients. This technology aims to reduce the need for re-sampling and save time by providing immediate feedback on whether the blood collection procedure was performed correctly or not.
          Traditionally, patients would send their blood sample cards to a lab for testing, only to find out later that the sample was insufficient or incorrect, requiring them to redo the blood collection process. This project addresses this issue by utilizing advanced computer vision algorithms to assess the blood samples in real time and identify potential errors, such as insufficient or excessive blood, incomplete filling, or other issues affecting the sample's usability.
          By providing immediate feedback to patients, this system will streamline the blood testing process, minimize the need for additional sampling, and improve overall patient satisfaction. Furthermore, this technology has the potential to enhance the efficiency of diagnostic labs, reducing the burden on healthcare professionals and ensuring more accurate results for patients. This algorithm was patented. 
        </p>
      </div>
    </div>


    <div class="col">
        <h3>FDA-Approved Robotic System for Rapid HbA1c Testing</h3>
        <a href="project1.html">
          <img src="https://soumilchugh.github.io/hba1c-test.jpg" alt="Project 1",id="project1img">
        </a>
        <p>
          This project introduces an FDA-approved robotic system capable of performing HbA1c tests in just 2 minutes. The system is based on a novel testing mechanism developed by JanaCare, which significantly expedites the testing process. The swift and efficient robotic system has the potential to be utilized in various settings, such as medical camps, where rapid testing is crucial.
          By automating the HbA1c testing process, this innovative system minimizes the likelihood of human errors, thereby enhancing the accuracy and reliability of the results. Its rapid testing capability not only provides quicker diagnoses for patients but also alleviates the workload for healthcare professionals, enabling them to focus on other crucial aspects of patient care.
        A demonstration of the designed system can be viewed at the following <a href="https://www.youtube.com/watch?v=dlgvvrSZJGU">link:</a>. This project represents a significant advancement in diagnostic technology, with the potential to revolutionize HbA1c testing and improve overall healthcare efficiency.
        </p>
      </div>
      <div class="col">
                
        <h3>Bluetooth-Enabled Insulin Pen Cap for Enhanced Diabetes Management</h3>
        <img src="https://soumilchugh.github.io/Insulin_pen.jpeg" alt="Project 2",id="project1img">
        <p>
          The focus of this project is the development of a universally compatible insulin pen cap equipped with Bluetooth connectivity. This innovative cap is designed to monitor and record essential information about insulin pen usage, such as the time of day it was utilized, the duration of use, and the temperature of the insulin stored within the pen.
        By collecting this data, the Bluetooth-enabled insulin pen cap offers valuable insights for patients and healthcare professionals alike, enabling more effective diabetes management. The gathered information can help users optimize their insulin administration routines, while healthcare professionals can use the data to make more informed treatment recommendations.
        Additionally, the cap's temperature monitoring capability ensures that the insulin is stored at the appropriate temperature, maintaining its efficacy and reducing potential wastage. This project aims to create a user-friendly and practical solution that not only simplifies diabetes management but also enhances the overall quality of care for patients living with this chronic condition.
        </p>
      </div>
    </div>


      <div class="col">
        <h3>Eye Tracker for a Head Mounted Headset</h3>
        <a href="project1.html">
          <img src="https://soumilchugh.github.io/vr.png" alt="Project 1",id="project1img">
        </a>
        <p>
          In head-mounted extended reality (XR) systems, eye tracking technology can unlock the potential for multifocal displays, foveated rendering, and enhanced interaction with 3D content. Achieving accurate gaze estimation at various fixation distances within the 3D scene is vital, as is maintaining precision when the device's position shifts relative to the user's head, which can alter the relationship between the eye-tracking apparatus and the eyes.
          The innovative hybrid approach developed involves the design of a video-based, head-mounted binocular eye-tracking system that leverages a 3D gaze estimation model and a convolutional neural network (CNN) based eye feature extractor. This method effectively addresses the challenges of maintaining accuracy during device movement and estimating gaze at different fixation distances. An impressive accuracy of 1 degree has been achieved, demonstrating the system's robustness. The project's findings have been documented and published in a research paper, contributing to the growing knowledge in this field.
        </p>
      </div>
      <div class="col">
                
        <h3>Eye Tracker for Desktops/Mobile Phones</h3>
        <img src="https://soumilchugh.github.io/phone.png" alt="Project 2",id="project1img">
        <p>
          Eye trackers generally rely on custom hardware, such as infrared cameras and light sources, for their design. To promote the widespread adoption and ubiquity of eye-tracking technology, it is crucial to develop algorithms compatible with standard RGB cameras found on desktop computers and smartphones. A neural network-based approach has been devised to estimate an individual's gaze direction using images captured by a low-resolution RGB camera. This method has successfully achieved an accuracy of 4 degrees, demonstrating its potential for broader applications. A patent was successfully filed.  
          The development of eye-tracking algorithms for standard RGB cameras not only expands the potential applications of this technology but also reduces implementation costs. By making eye tracking more accessible, it opens up possibilities for its use in various fields, such as gaming, virtual reality, marketing, and accessibility features for individuals with disabilities. This innovation has the potential to revolutionize user experiences across multiple industries.
        </p>
      </div>
    </div>


    

  </div>
</body>
</html>        
